ğŸš€ Vector Database & Retrieval Performance Comparison (ChromaDB vs Pinecone)
ğŸ“Œ Overview
This project provides a practical benchmarking framework for comparing:

Traditional lexical retrieval (BM25)

Vector-based semantic search using HNSW indexing

across:

âœ… ChromaDB (local open-source vector database)
âœ… Pinecone (managed cloud vector database)

A collection of PDF documents is processed into text chunks, embedded using fast SentenceTransformer models, and evaluated based on:

â± Query latency (average response time)

ğŸ“ˆ Retrieval quality (heuristic scoring)

The goal is to understand speed vs quality trade-offs between local and cloud vector stores.

ğŸ“‚ Dataset
Source: PDF documents (user-provided folder)

Preprocessing Pipeline:
Text extracted using pypdf

Chunked with overlap for semantic continuity

Embedded using transformer models

Query type: Natural language queries
Top-K results: 5
Similarity metric: Cosine similarity

âš ï¸ PDFs are excluded from the repository for size reasons.

ğŸ§  Embedding Models Used
Alias	Model Name	Characteristics
MiniLM-L3	paraphrase-MiniLM-L3-v2	Lightweight & fast
DIST	distilbert-base-nli-stsb-mean-tokens	Balanced quality
(Optimized for performance benchmarking)

ğŸ—„ï¸ Vector Databases Compared
Database	Type	Deployment
ChromaDB	Open-source	Local persistent storage
Pinecone	Managed	Serverless cloud
ğŸ” Retrieval Methods
Algorithm	Category
BM25	Lexical keyword search
HNSW	Approximate Nearest Neighbor (vector search)                                                                                 

âš™ï¸ Evaluation Configuration
Queries: 20 natural language questions
Top-K retrieval: 5
Similarity metric: Cosine similarity
Metrics evaluated:
Average response time (ms)
Qualitative retrieval accuracy


 Performance Results
ğŸ”¹ Benchmark Summary
Configuration	Embedding Model	Avg Latency (ms)	Accuracy Level
BM25	N/A	210.93	Medium
Brute Force	MiniLM	21.43	High
HNSW (ChromaDB)	MiniLM	5.47	High
HNSW (Pinecone)	MiniLM	539.73	Very High


ğŸ§  Key Observations
BM25 is slower and less accurate for semantic queries.
Brute-force vector search improves accuracy but scales poorly.
ChromaDB (HNSW) provides the lowest latency for local workloads.
Pinecone delivers very high retrieval quality but incurs higher latency due to network and serverless overhead.
Both ChromaDB and Pinecone require batched ingestion for large datasets due to internal limits.

ğŸ—ï¸ Project Structure
.
â”œâ”€â”€ ChromaDb.py                  # ChromaDB benchmarking pipeline
â”œâ”€â”€ PineCode.py                  # Pinecone benchmarking pipeline
â”œâ”€â”€ compare_CDB_PI.py            # Aggregates final results
â”œâ”€â”€ chroma_db/                  # Chroma persistent storage
â”œâ”€â”€ chroma_performance_results.pkl
â”œâ”€â”€ pinecone_performance_results.pkl
â”œâ”€â”€ finalresult.pkl
â””â”€â”€ README.md


ğŸš€ Conclusion

This project demonstrates that:

ğŸ‘‰ Vector-based retrieval systems significantly outperform traditional BM25 for semantic queries
ğŸ‘‰ Local vector databases like ChromaDB excel in low-latency scenarios
ğŸ‘‰ Cloud platforms like Pinecone offer robust indexing but may introduce network overhead

Overall, HNSW-based vector search is the optimal approach for modern RAG and AI retrieval pipelines.

ğŸ‘¨â€ğŸ’» Author

Jaya Bharath
